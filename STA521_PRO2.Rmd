---
title: "STA521_PRO2"
author: "Sijie Chen"
date: "10/31/2021"
output: html_document
---

```{r}
library(tidyverse)
library(ggplot2)
library(tidyverse)
library(reshape)
library(GGally)
library(DiscriMiner)
library(gridExtra)
library(caret)
library(glmnet)
library(corrplot)
library(pROC)
library(xgboost)
```


#1 Data Collection and Exploration

```{r}
#load data
image1 <- as.data.frame(read.csv('imagem1.txt',sep='',header=FALSE))
image2 <- read.csv('imagem2.txt', sep='',header=FALSE)
image3 <- read.csv('imagem3.txt', sep='',header=FALSE)


vcol <- c('y','x','expert_labels','NDAI','SD','CORR','DF','CF','BF','AF','AN')



colnames(image1) <- vcol
colnames(image2) <- vcol
colnames(image3) <- vcol

# combine all three datasets
image <- rbind(image1,image2,image3)
```



## (b)
According to the heat map, we can observe patterns between xy coordinates and expert labels. The majority of unlabeled pixels are around x coordinates near 300. Furthermore, we can clearly see that the closer the geological location of the pixels are, they are more likely to share same label. Therefore, our assumption of independent and identical distributed doesn't hold for this data set. 
```{r}
#summarize the data
table(image$expertlabels)
ggplot(data=image,aes(x=x,y=y,fill=expert_labels))+
  geom_tile()+
  ggtitle('Heat Map With Exexpert Labels')
```

## (c) Perform visual and quantitative and visual EDA
### pairwise relationships
```{r}
#image %>% 
  #select(NDAI,SD,CORR,DF,CF,BF,AF,AN) %>% 
  #ggpairs()
```
Among all eight feature correlation, we found that there are seven pairs correlations between features are higher than $0.7$. 
```{r}

imagefeature <- image %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.7) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}

```
### pairwise relationships between expert labels and individual features.

We suspects that the underlying relationship between expert labels and individual features are not linear. For better estimation on correlation between continuous variable and categorical variable, we used correlation ration. According to the quantitative result, NDAI and CORR have highest correlation with expert labels. SD togther with radiance angle AF and AN approximately have equal correlation with expert labels that is around $0.15$.

```{r}
for (i in 1:ncol(imagefeature)){
  print(corRatio(imagefeature[,i],image$expert_labels))
}
```
According to visualization, distribution of features with highest five correlation ratio under three labels are distinct meaning those features are valuable when we creating the model. 
```{r}
#change expertlabels into factors
image <- image %>% 
  mutate(expertlabels=as.factor(expert_labels))
#visualization on correlations
g1 <- ggplot(image,aes(x=NDAI,fill=expert_labels))+
    geom_density(alpha=0.3)
g2 <- ggplot(image,aes(x=SD,fill=expert_labels))+
    geom_density(alpha=0.3)
g3 <- ggplot(image,aes(x=CORR,fill=expert_labels))+
    geom_density(alpha=0.3)
g4 <- ggplot(image,aes(x=AF,fill=expert_labels))+
    geom_density(alpha=0.3)
g5 <- ggplot(image,aes(x=AN,fill=expert_labels))+
    geom_density(alpha=0.3)
grid.arrange(g1,g2,g3,g4,g5,ncol=2)
```

In addition, we want to see the differences between two classes (cloud,no cloud) based on features(CORR,NADI and SD).

```{r}
#specificdf only contains two classes and three features
specificdf <- image %>% 
  dplyr::select(CORR,NDAI,SD,expert_labels) %>% 
  filter(expert_labels==1|expert_labels==-1)
```

```{r}
sp1 <- ggplot(specificdf,aes(x=CORR,fill=expert_labels))+
  geom_density(alpha=0.3)
sp11 <- sp1+facet_wrap(expert_labels~.)
sp2 <- ggplot(specificdf,aes(x=NDAI,fill=expert_labels))+
  geom_density(alpha=0.3)
sp22 <- sp2+facet_wrap(expert_labels~.)
sp3 <- ggplot(specificdf,aes(x=SD,fill=expert_labels))+
  geom_density(alpha=0.3)
sp33 <- sp3+facet_wrap(expert_labels~.)

grid.arrange(sp11,sp22,sp33)
```

According to summary statistics for features CORR, NDAI and SD grouped by expert labels, for pixels labeled as cloud, on average, the pixel data point has higher CORR, NDAI and SD. This fact agrees with our previous visualization which mode of three features in cloud group are statistically larger than modes in no cloud group. 

```{r}
#quant
specificdf %>% 
  group_by(expert_labels) %>% 
  summarize(meanCORR = mean(CORR), meanNDAI= mean(NDAI), meanSD= mean(SD),
            sdCORR = sd(CORR), sdNDAI= sd(NDAI), sdSD= sd(SD))
```


# 2 Preparation

## (a) Train Test Split
```{r}
df <- rbind(image1,image2,image3)
data = df%>%filter(expert_labels != 0)
cloud = data%>%filter(expert_labels == 1)
nocloud = data%>%filter(expert_labels == 1)

imagefeature <- image %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
print("----------------")

imagefeature <- cloud %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
print("----------------")
imagefeature <- nocloud %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
typeof(df)
```


```{r}
# split along y 
data <- data%>%mutate(block_y = ((y-1)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_y %in% train_index)
test_data = data%>%filter(!(block_y %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_y %in%validation_index)
train_data

# split along x
data = df%>%filter(expert_labels != 0)
data <- data%>%mutate(block_x = ((x-64)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_x %in% train_index)
test_data = data%>%filter(!(block_x %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_x %in%validation_index)


# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = ((y-1)%%10)+1,block_x = ((x-64)%%10)+1)%>%mutate(block_index = (block_y-1)*10+block_x)
table(data$block_index)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)


# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = floor(y/39)+1,block_x = floor(x/31)-2)%>%mutate(block_index = (block_y-1)*10+block_x)
table(data$block_index)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)

table(train_data$expert_labels)

train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
```



# b 
```{r}
test_data
# test classifications error 
1 - sum(test_data$expert_labels == 0)/length(test_data$expert_labels)
# validation classifications error
sum(validation_data$expert_labels == 0)/length(validation_data$expert_labels)
1 - sum(validation_data$expert_labels == 0)/length(validation_data$expert_labels)
```

# c
In order to check important variables associated with expert_labels, we can use a logistic regression. 
```{r}
featuredf <- select(train_data,-c(x,y,block_y,block_x,block_index))
#label = 1 refers to cloud, 0 refers to no cloud
featuredf$expert_labels <- ifelse(featuredf$expert_labels==1,1,0)

logit <- glm(expert_labels~.,data=featuredf,family='binomial')
#xtest <- data.matrix(subset(featuredf,select=-c(expert_labels)))
#ytest <- data.matrix(subset(featuredf,select=c(expert_labels)))
#logit <- glmnet(xtest,ytest,family='binomial',alpha=1,lambda=NULL)
```

The logistic model summary output for all features have p value smaller than $0.05$. 
Nevertheless, we can use variable importance function to perform feature selection. As the result of the varImp, we have the highest four variable importance value which are features: NDAI, SD, CORR and DF. 
```{r}
summary(logit)
```

The visualization of correlation matrix conveys the message that the information from different angles are high correlated, which means they are redundant variables. Furthermore, although variable DF shows descent variable importance value, it has a relatively low (close to zero) correlation with expert labels, thus we will exclude the variable from future modeling. 
```{r}
cor(featuredf)
corrplot(cor(featuredf))
```


```{r}
theme_update(plot.title = element_text(hjust = 0.5))
VI <- varImp(logit,scale=FALSE)
ggplot(aes(x=rownames(VI),y=Overall),data=VI)+
  geom_point()+
  ggtitle('Variable Importance Plot')
```

```{r}
#ggplot(featuredf, aes(x=CORR, y=expert_labels)) + 
  #geom_point(alpha=.5) +
  #stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial),
              #col="red", lty=2)
```

## (d)
```{r cv function}
set.seed(521)
fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5,
                           )
#training_data = cbind(features,labels
# svm_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
#                  method = "svmLinear", 
#                  trControl = fitControl,
#                  preProcess = c("center","scale"))
                 

```


#3 Modeling

## a
```{r}
#run logistic on the whole training dataset using block train test split
train_data$expert_labels = as.factor(train_data$expert_labels)
log_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "glm", 
                 trControl = fitControl,
                 family = "binomial"
                 )
summary(log_fit)
predicted <- predict(log_fit,test_data)
sum(predicted == test_data$expert_labels)/length(test_data$expert_labels)
```


```{r experiment}
# this mini data is only for experiment
n = nrow(data)
mini_data_index = sample(c(1:n), 10000)
mini_data = data[mini_data_index,]
mini_data$expert_labels = as.factor(mini_data$expert_labels)

#mini_data = df%>%filter(expert_labels != 0)
mini_data <- mini_data%>%mutate(block_y = ((y-1)%%10)+1,block_x = ((x-64)%%10)+1)%>%mutate(block_index = (block_y-1)*10+block_x)
table(mini_data$block_index)
train_index = sample(1:100,80)
train_data = mini_data%>%filter(block_index %in% train_index)
test_data = mini_data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)

table(train_data$expert_labels)

mini_data
```



```{r}
set.seed(521)
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10
                           )
svm_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "svmLinear", 
                 trControl = fitControl,
                 tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
                 preProcess = c("center","scale"))
```

```{r}
summary(svm_fit)
svm_fit$results
predict_val = predict(svm_fit,newdata = test_data)
sum(predict_val == test_data$expert_labels)/nrow(test_data)
```

```{r}
validation_data$expert_labels
predict_val
```
```{r}
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')
rf_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "rf", 
                 metric = 'Accuracy',
                   #tuneLength  = 15, 
                 #  trControl = control
                )

```

```{r}
library(randomForest)
rf_fit <- randomForest(expert_labels ~ NDAI + SD +CORR + DF , data = train_data, ntree = 20, mtry = 4, importance = T)
plot(rf_fit)


```

```{r}
control <- trainControl(method='cv', 
                        number=10, 
                        repeats=3,
                        search = 'random')
rf_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "nnet", 
                 metric = 'Accuracy',
                   #tuneLength  = 15, 
                  trControl = control
              #  linout = T
                )
```


```{r}
rf_fit$results
```

```{r}
predicted = predict(rf_fit,test_data)
sum(predicted == test_data$expert_labels)/nrow(test_data)
```

```{r}
#try lda and qda on the whole dataset
data$expert_labels <- ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = ((y-1)%%10)+1,block_x = ((x-64)%%10)+1)%>%mutate(block_index = (block_y-1)*10+block_x)
table(data$block_index)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)
```

```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10
                           )
train_data$expert_labels <- as.factor(train_data$expert_labels)
lda_fit <- train(expert_labels ~ NDAI + SD + CORR, data = train_data, 
                 method = "lda", 
                 trControl = fitControl,
                 #tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
                 preProcess = c("center","scale"))

```

```{r}
lda_fit
```


```{r}
fitControl <- trainControl(## 10-fold CV
                           method = "cv",
                           number = 10,
                           repeats = 3
                           )
test_data$expert_labels <- as.factor(test_data$expert_labels)
train_data$expert_labels <- as.factor(train_data$expert_labels)
qda_fit <- train(expert_labels ~ NDAI + SD + CORR, data = train_data, 
                 method = "qda", 
                 trControl = fitControl,
                 #tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
                 preProcess = c("center","scale"))
```

```{r}
rs <- predict(qda_fit,test_data)
sum(rs == test_data$expert_labels)/ nrow(test_data)
```

## b ROC curves
```{r}
#ROC for logistic
test_prob = predict(logit,)
```


```{r}
sparse_matrix <- sparse.model.matrix(expert_labels ~ NDAI + SD + CORR ~ .-1, data = train_data)
```

```{r}
set.seed(521)
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=2
)
xgb.train = xgb.DMatrix(data=as.matrix(train_data[,4:7]),label=train_data$expert_labels)
xgb.test = xgb.DMatrix(data=as.matrix(test_data[,4:7]),label=test_data$expert_labels)
xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 0.01,
 max_depth = 4, 
 nround=50, 
 subsample = 0.5,
 colsample_bytree = 1,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 verbose = T
)
(train_data$expert_labels)
dim(train_data[,4:7])
length(train_data$expert_labels)

train_data
```

```{r}
y_pred <- predict(xgb, data.matrix(test_data[4:7]))
y_pred[y_pred >0.5] = 1
y_pred[y_pred <= 0.5] = 0
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
```

```{r}
plot(xgb$evaluation_log,type = "l")
```



```{r}
set.seed(521)
max_depths = c(1:15)
errors = c()
for(depths in max_depths){
  xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 0.5,
 gamma = 1,
 max_depth = depths, 
 nround=50, 
 subsample = 0.1,
 colsample_bytree = 0.5,
 eval_metric = "error",
 objective = "binary:logistic",
 verbose = F,
 nthread = 1)
  y_pred <- predict(xgb, data.matrix(test_data[4:7]))
  y_pred[y_pred >0.5] = 1
  y_pred[y_pred <= 0.5] = 0
  error = 1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
  errors = c(errors,error)
}
errors
plot(errors,type = "l")
```

```{r}
train_data
```


```{r}
library(keras)
#library(EBImage)
library(stringr)
library(pbapply)
library(tensorflow)
use_condaenv("keras-tf", required = T)
train_array %>%
layer_dropout(rate = 0.25) %>%
layer_flatten() %>%
layer_dense(units = 50, activation = "relu") %>%
layer_dropout(rate = 0.25) %>%
layer_dense(units = 1, activation = "sigmoid")
```


```{python}
import tensorflow as tf
```
```{r}
# load library
require(neuralnet)

# fit neural network
nn=neuralnet(expert_labels ~ NDAI + SD + CORR,data=train_data, hidden=3,act.fct = "logistic",
                linear.output = T)
```

```{r}
plot(nn)
pred = compute(nn,test_data)
prob = pred$net.result[,2]
pred =ifelse(prob>0.5, 1, 0)
sum(pred == test_data$expert_labels)/length(test_data$expert_labels)
```


```{r cv master function}
library(MASS)
CVmaster = function(classifier,features,labels,nfold = 5){
  #fitControl <- trainControl(## 5-fold CV
                           #method = "cv",
                           #number = nfold)
  folds <- createFolds(labels[[1]], k = nfold)
  fold_errors = c()
  i = 1
  if (classifier=='lda'){
   for(fold in folds){
     train_feature = features[-fold,]
     test_feature = features[fold,]
     train_label = as.factor(labels[-fold,])
     test_label = as.factor(labels[fold,])
     train_set = cbind(train_feature,train_label)
     t = cbind(test_feature,test_label)
     modellda = lda(train_label ~ ., newdata=train_set)
     predict_label = predict(modellda,data=t)
     print(length(predict_label$class))
     print(length(test_label))
     error = (sum(predict_label$class == test_label))/length(test_label)
     #print(predict_label$class )
     #print(unique(test_label))
     fold_errors = c(fold_errors,error)
     print(paste0('This is fold', i))
     i = i+1
  }
  return(fold_errors)
  }
}
```

```{r for cv master}
#cv master on lda regression
traindata <- rbind(train_data,validation_data)
features = traindata %>% 
  dplyr::select(-expert_labels) %>% 
  dplyr::select(NDAI,SD,CORR)
labels = traindata %>% 
  dplyr::select(expert_labels) 
#labels = as.factor(labels[[1]])
CVmaster(classifier='lda', features=features, labels=labels,nfold=5)
labels
```





# KNN 

## knn with selected 4 features 
```{r}
library(class)

# knn data rescale
train_knn = scale(train_data[,4:7],center = TRUE,scale = TRUE)
test_knn = scale(test_data[,4:7],center = TRUE,scale = TRUE)
train_labels = train_data$expert_labels
knn.26 <- knn(train=train_knn, test=test_knn, cl=train_data$expert_labels, k=26)

sum(knn.26 == test_data$expert_labels)/length(test_data$expert_labels)


cv_knn = function(train,label,k = 5,nfold = 5){
  folds <- createFolds(label, k = nfold, list = TRUE, returnTrain = FALSE)
  errors = c()
  idx = 1
  for(fold in folds){
    train_curr = train[-fold,]
    test_curr = train[fold,]
    knn_curr = knn(train = train_curr,test = test_curr,cl = label[-fold],k = k)
    error = 1- sum(knn_curr == label[fold])/length(label[fold])
    errors = c(errors,error)
    print(paste0("Fold ",idx," in session"))
    print(paste0("Fold error is ",error))
    idx = idx + 1
  }
  return(mean(errors))
  
}

cv_knn(train_knn,train_labels)

folds <- createFolds(labels_knn, k = 10, list = TRUE, returnTrain = FALSE)

errors = c()
for(fold in folds){
  train_curr = train_knn[-fold,]
  test_curr = train_knn[fold,]
  knn_curr = knn(train=train_curr, test=test_curr, cl=labels_knn[-fold], k=26)
  error = 1- sum(knn_curr == labels_knn[fold])/length(labels_knn[fold])
  errors = c(errors,error)
}
mean(errors)

ks = c(1:10)
cv_erros = c()
for(k in ks){
  error = cv_knn(train_knn,train_labels,k = k,nfold = 5)
  cv_erros =c(cv_erros,error)
}
plot(ks,cv_erros,type = "l",main = "KNN test loss",ylab = "Test loss", xlab = "Choice of K")


```
## pca
```{r}
library(stats)
set.seed(521)
pca_data = scale(train_data[,4:11],scale = T,center = T)
pca = prcomp(pca_data)
pca$x



#select first 4 pcs 
pca_dataset = pca$x[,1:4]
#xgboost
xgb <- xgboost(data = as.matrix(pca_dataset), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 1,
 max_depth = 4, 
 nround=200, 
 subsample = 0.5,
 colsample_bytree = 0.5,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 verbose = T
)
# form pca test set
pca_data_test = scale(test_data[,4:11],scale = T,center = T)
test_pca = prcomp(pca_data_test)$x
test_pca_dataset = test_pca[,1:4]

y_pred <- predict(xgb, test_pca_dataset)
y_pred <- ifelse(y_pred > 0.5,1,0)
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)



# use first four features(conclusion from eda)
xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 1,
 max_depth = 4, 
 nround=50, 
 subsample = 0.5,
 colsample_bytree = 0.5,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 early_stop_rounds = 3,
 verbose = T
)
xgb
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)


## knn 
cv_knn(pca_dataset,train_labels)
cv_knn(train_knn,train_labels)
```

## tuning for xgboost


# 1 
min_child_weight:minimum sum of weights of all observations required in a child
If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. Alternatively, it can be viewed as a commend for algorithm to stop split when node reach a certain level of purity. An useful reference post \:https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm

max_depth: maximium depth of a tree can get

Both of these two parameters prevent overfiting by stop tree splitting at certain level. Hence, they are tuned toghter here.



```{r}
set.seed(521)
cv_xgboost = function(train_data,label,max_depth = 4,min_child = 1,nfold = 5){
  folds <- createFolds(label, k = nfold, list = TRUE, returnTrain = FALSE)
  errors = c()
  for(fold in folds){
    train_curr = train_data[-fold,]
    test_curr = train_data[fold,]
    label_curr = label[-fold]
    xgb <- xgboost(data = train_curr, 
      label = label_curr, 
      eta = 1,
      max_depth = max_depth, 
      min_child_weight = min_child,
      nround=50, 
      subsample = 0.5,
      colsample_bytree = 0.5,
      eval_metric = "error",
      objective = "binary:logistic",
      nthread = 1,
      verbose = F
    )
    y_pred <- predict(xgb, data.matrix(test_curr))
    y_pred <- ifelse(y_pred > 0.5,1,0)
    error = 1-  sum(y_pred == label[fold])/length(label[fold])
    errors = c(errors,error)
  }
  return(mean(errors))
}

cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1)
max_depth = c(3,5,7,9,11)
min_child = c(1,3,5)

res = c()
for(d in max_depth){
  for(c in min_child){
    error = cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1,max_depth = d,min_child = c)
    res = c(res,error)
    print(paste0("max depths: ",d," mini_child_weight: ",c," ",": ",error))
  }
}

xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
      label = as.numeric(train_data$expert_labels) - 1, 
      eta = 1,
      max_depth = 3, 
      min_child_weight = 5,
      nround=50, 
      subsample = 0.5,
      colsample_bytree = 0.5,
      eval_metric = "error",
      objective = "binary:logistic",
      nthread = 1,
      verbose = T
    )
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
min(res)
```


# 2 gamma
A node is split only when the resulting split gives a positive reduction in the loss function. Gamma specifies the minimum loss reduction required to make a split.
```{r}

```

