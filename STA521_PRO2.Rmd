---
title: "STA521_PRO2"
author: "Sijie Chen"
date: "10/31/2021"
output: html_document
---

```{r}
library(tidyverse)
library(ggplot2)
library(tidyverse)
library(reshape)
library(GGally)
library(DiscriMiner)
library(gridExtra)
library(caret)
library(glmnet)
library(corrplot)
library(pROC)
library(xgboost)
library(MASS)
library(class)
```


#1 Data Collection and Exploration

```{r}
#load data
image1 <- as.data.frame(read.csv('imagem1.txt',sep='',header=FALSE))
image2 <- read.csv('imagem2.txt', sep='',header=FALSE)
image3 <- read.csv('imagem3.txt', sep='',header=FALSE)


vcol <- c('y','x','expert_labels','NDAI','SD','CORR','DF','CF','BF','AF','AN')



colnames(image1) <- vcol
colnames(image2) <- vcol
colnames(image3) <- vcol

# combine all three datasets
image <- rbind(image1,image2,image3)
```



## (b)
According to the heat map, we can observe patterns between xy coordinates and expert labels. The majority of unlabeled pixels are around x coordinates near 300. Furthermore, we can clearly see that the closer the geological location of the pixels are, they are more likely to share same label. Therefore, our assumption of independent and identical distributed doesn't hold for this data set. 
```{r}
#summarize the data
table(image$expertlabels)
ggplot(data=image,aes(x=x,y=y,fill=expert_labels))+
  geom_tile()+
  ggtitle('Heat Map With Exexpert Labels')
```

## (c) Perform visual and quantitative and visual EDA
### pairwise relationships
```{r}
#image %>% 
  #select(NDAI,SD,CORR,DF,CF,BF,AF,AN) %>% 
  #ggpairs()
```
Among all eight feature correlation, we found that there are seven pairs correlations between features are higher than $0.7$. 
```{r}
imagefeature <- image %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.7) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}

```
### pairwise relationships between expert labels and individual features.

We suspects that the underlying relationship between expert labels and individual features are not linear. For better estimation on correlation between continuous variable and categorical variable, we used correlation ration. According to the quantitative result, NDAI and CORR have highest correlation with expert labels. SD togther with radiance angle AF and AN approximately have equal correlation with expert labels that is around $0.15$.

```{r}
for (i in 1:ncol(imagefeature)){
  print(corRatio(imagefeature[,i],image$expert_labels))
}
```
According to visualization, distribution of features with highest five correlation ratio under three labels are distinct meaning those features are valuable when we creating the model. 
```{r}
#change expertlabels into factors
image <- image %>% 
  mutate(expertlabels=as.factor(expert_labels))
#visualization on correlations
g1 <- ggplot(image,aes(x=NDAI,fill=expert_labels))+
    geom_density(alpha=0.3)
g2 <- ggplot(image,aes(x=SD,fill=expert_labels))+
    geom_density(alpha=0.3)
g3 <- ggplot(image,aes(x=CORR,fill=expert_labels))+
    geom_density(alpha=0.3)
g4 <- ggplot(image,aes(x=AF,fill=expert_labels))+
    geom_density(alpha=0.3)
g5 <- ggplot(image,aes(x=AN,fill=expert_labels))+
    geom_density(alpha=0.3)
grid.arrange(g1,g2,g3,g4,g5,ncol=2)
```

In addition, we want to see the differences between two classes (cloud,no cloud) based on features(CORR,NADI and SD).

```{r}
#specificdf only contains two classes and three features
specificdf <- image %>% 
  dplyr::select(CORR,NDAI,SD,expert_labels) %>% 
  filter(expert_labels==1|expert_labels==-1)
```

```{r}
sp1 <- ggplot(specificdf,aes(x=CORR,fill=expert_labels))+
  geom_density(alpha=0.3)
sp11 <- sp1+facet_wrap(expert_labels~.)
sp2 <- ggplot(specificdf,aes(x=NDAI,fill=expert_labels))+
  geom_density(alpha=0.3)
sp22 <- sp2+facet_wrap(expert_labels~.)
sp3 <- ggplot(specificdf,aes(x=SD,fill=expert_labels))+
  geom_density(alpha=0.3)
sp33 <- sp3+facet_wrap(expert_labels~.)

grid.arrange(sp11,sp22,sp33)
```

According to summary statistics for features CORR, NDAI and SD grouped by expert labels, for pixels labeled as cloud, on average, the pixel data point has higher CORR, NDAI and SD. This fact agrees with our previous visualization which mode of three features in cloud group are statistically larger than modes in no cloud group. 

```{r}
#quant
specificdf %>% 
  group_by(expert_labels) %>% 
  summarize(meanCORR = mean(CORR), meanNDAI= mean(NDAI), meanSD= mean(SD),
            sdCORR = sd(CORR), sdNDAI= sd(NDAI), sdSD= sd(SD))
```


# 2 Preparation

## (a) Train Test Split
```{r}
df <- rbind(image1,image2,image3)
data = df%>%filter(expert_labels != 0)
cloud = data%>%filter(expert_labels == 1)
nocloud = data%>%filter(expert_labels == 1)

imagefeature <- image %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
print("----------------")

imagefeature <- cloud %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
print("----------------")
imagefeature <- nocloud %>% 
  dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
    correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
    if(length(correlations)> 0){
      lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
    }
}
typeof(df)
```


```{r}
set.seed(521)
# split along y 
data <- data%>%mutate(block_y = ((y-1)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_y %in% train_index)
test_data = data%>%filter(!(block_y %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_y %in%validation_index)
train_data

# split along x
data = df%>%filter(expert_labels != 0)
data <- data%>%mutate(block_x = ((x-64)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_x %in% train_index)
test_data = data%>%filter(!(block_x %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_x %in%validation_index)




# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = floor(y/39)+1,block_x = floor(x/31)-2)%>%mutate(block_index = (block_y-1)*10+block_x)
table(data$block_index)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)

table(train_data$expert_labels)

train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
```



# b 
```{r}
test_data
# test classifications error 
1 - sum(test_data$expert_labels == 0)/length(test_data$expert_labels)
# validation classifications error
sum(validation_data$expert_labels == 0)/length(validation_data$expert_labels)
1 - sum(validation_data$expert_labels == 0)/length(validation_data$expert_labels)
```

# c
In order to check important variables associated with expert_labels, we can use a logistic regression. 
```{r}
featuredf <- select(train_data,-c(x,y,block_y,block_x,block_index))
#label = 1 refers to cloud, 0 refers to no cloud
featuredf$expert_labels <- ifelse(featuredf$expert_labels==1,1,0)

logit <- glm(expert_labels~.,data=featuredf,family='binomial')
#xtest <- data.matrix(subset(featuredf,select=-c(expert_labels)))
#ytest <- data.matrix(subset(featuredf,select=c(expert_labels)))
#logit <- glmnet(xtest,ytest,family='binomial',alpha=1,lambda=NULL)
```

The logistic model summary output for all features have p value smaller than $0.05$. 
Nevertheless, we can use variable importance function to perform feature selection. As the result of the varImp, we have the highest four variable importance value which are features: NDAI, SD, CORR and DF. 
```{r}
summary(logit)
```

The visualization of correlation matrix conveys the message that the information from different angles are high correlated, which means they are redundant variables. Furthermore, although variable DF shows descent variable importance value, it has a relatively low (close to zero) correlation with expert labels, thus we will exclude the variable from future modeling. 
```{r}
cor(featuredf)
corrplot(cor(featuredf))
```


```{r}
theme_update(plot.title = element_text(hjust = 0.5))
VI <- varImp(logit,scale=FALSE)
ggplot(aes(x=rownames(VI),y=Overall),data=VI)+
  geom_point()+
  ggtitle('Variable Importance Plot')
```

```{r}
#ggplot(featuredf, aes(x=CORR, y=expert_labels)) + 
  #geom_point(alpha=.5) +
  #stat_smooth(method="glm", se=FALSE, method.args = list(family=binomial),
              #col="red", lty=2)
```

## (d)
```{r cv function}
set.seed(521)
fitControl <- trainControl(## 5-fold CV
                           method = "cv",
                           number = 5,
                           )
#training_data = cbind(features,labels
# svm_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
#                  method = "svmLinear", 
#                  trControl = fitControl,
#                  preProcess = c("center","scale"))
                 

```


#3 Modeling

## a
```{r}
#run logistic on the whole training dataset using block train test split
train_data$expert_labels = as.factor(train_data$expert_labels)
log_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "glm", 
                 trControl = fitControl,
                 family = "binomial"
                 )
summary(log_fit)
predicted <- predict(log_fit,test_data)
sum(predicted == test_data$expert_labels)/length(test_data$expert_labels)
```


```{r experiment}
# this mini data is only for experiment
n = nrow(data)
mini_data_index = sample(c(1:n), 10000)
mini_data = data[mini_data_index,]
mini_data$expert_labels = as.factor(mini_data$expert_labels)

#mini_data = df%>%filter(expert_labels != 0)
mini_data <- mini_data%>%mutate(block_y = ((y-1)%%10)+1,block_x = ((x-64)%%10)+1)%>%mutate(block_index = (block_y-1)*10+block_x)
table(mini_data$block_index)
train_index = sample(1:100,80)
train_data = mini_data%>%filter(block_index %in% train_index)
test_data = mini_data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)

table(train_data$expert_labels)

mini_data
```



```{r}
# set.seed(521)
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 10
#                            )
# svm_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
#                  method = "svmLinear", 
#                  trControl = fitControl,
#                  tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
#                  preProcess = c("center","scale"))
```

```{r}
# summary(svm_fit)
# svm_fit$results
# predict_val = predict(svm_fit,newdata = test_data)
# sum(predict_val == test_data$expert_labels)/nrow(test_data)
```


```{r}
control <- trainControl(method='repeatedcv', 
                        number=10, 
                        repeats=3,
                        search = 'random')
rf_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
                 method = "rf", 
                 metric = 'Accuracy',
                   #tuneLength  = 15, 
                 #  trControl = control
                )

```

```{r}
library(randomForest)
rf_fit <- randomForest(expert_labels ~ NDAI + SD +CORR + DF , data = train_data, ntree = 20, mtry = 4, importance = T)
plot(rf_fit)


```

```{r}
# control <- trainControl(method='cv', 
#                         number=10, 
#                         repeats=3,
#                         search = 'random')
# rf_fit <- train(expert_labels ~ NDAI + SD + CORR + DF, data = train_data, 
#                  method = "nnet", 
#                  metric = 'Accuracy',
#                    #tuneLength  = 15, 
#                   trControl = control
#               #  linout = T
#                 )
```


```{r}
rf_fit$results
```

```{r}
predicted = predict(rf_fit,test_data)
sum(predicted == test_data$expert_labels)/nrow(test_data)
```

```{r}
#try lda and qda on the whole dataset
# data$expert_labels <- ifelse(data$expert_labels==1,1,0)
# data <- data%>%mutate(block_y = ((y-1)%%10)+1,block_x = ((x-64)%%10)+1)%>%mutate(block_index = (block_y-1)*10+block_x)
# table(data$block_index)
# train_index = sample(1:100,80)
# train_data = data%>%filter(block_index %in% train_index)
# test_data = data%>%filter(!(block_index %in% train_index))
# validation_index = sample(train_index,10)
# validation_data = train_data%>%filter(block_index %in%validation_index)
```

```{r ldanocv}
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 10
#                            )
# train_data$expert_labels <- as.factor(train_data$expert_labels)
# lda_fit <- train(expert_labels ~ NDAI + SD + CORR, data = train_data, 
#                  method = "lda", 
#                  trControl = fitControl,
#                  #tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
#                  preProcess = c("center","scale"))

```

```{r qdanocv}
# fitControl <- trainControl(## 10-fold CV
#                            method = "cv",
#                            number = 10,
#                            repeats = 3
#                            )
# test_data$expert_labels <- as.factor(test_data$expert_labels)
# train_data$expert_labels <- as.factor(train_data$expert_labels)
# qda_fit <- train(expert_labels ~ NDAI + SD + CORR, data = train_data, 
#                  method = "qda", 
#                  trControl = fitControl,
#                  #tuneGrid = expand.grid(C = seq(0.2, 2, length = 5)),
#                  preProcess = c("center","scale"))
```

```{r qda assess test error}
# rs <- predict(qda_fit,test_data)
# sum(rs == test_data$expert_labels)/ nrow(test_data)
```

## b ROC curves
```{r}
#ROC for logistic
test_prob = predict(logit,)
```


```{r}
sparse_matrix <- sparse.model.matrix(expert_labels ~ NDAI + SD + CORR ~ .-1, data = train_data)
```

```{r}
set.seed(521)
params = list(
  booster="gbtree",
  eta=0.001,
  max_depth=5,
  gamma=3,
  subsample=0.75,
  colsample_bytree=1,
  objective="multi:softprob",
  eval_metric="mlogloss",
  num_class=2
)
xgb.train = xgb.DMatrix(data=as.matrix(train_data[,4:7]),label=train_data$expert_labels)
xgb.test = xgb.DMatrix(data=as.matrix(test_data[,4:7]),label=test_data$expert_labels)
xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 0.01,
 max_depth = 4, 
 nround=50, 
 subsample = 0.5,
 colsample_bytree = 1,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 verbose = T
)
(train_data$expert_labels)
dim(train_data[,4:7])
length(train_data$expert_labels)


```

```{r}
y_pred <- predict(xgb, data.matrix(test_data[4:7]))
y_pred[y_pred >0.5] = 1
y_pred[y_pred <= 0.5] = 0
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
```

```{r}
plot(xgb$evaluation_log,type = "l")
```



```{r}
set.seed(521)
max_depths = c(1:15)
errors = c()
for(depths in max_depths){
  xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 0.5,
 gamma = 1,
 max_depth = depths, 
 nround=50, 
 subsample = 0.1,
 colsample_bytree = 0.5,
 eval_metric = "error",
 objective = "binary:logistic",
 verbose = F,
 nthread = 1)
  y_pred <- predict(xgb, data.matrix(test_data[4:7]))
  y_pred[y_pred >0.5] = 1
  y_pred[y_pred <= 0.5] = 0
  error = 1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
  errors = c(errors,error)
}
errors
plot(errors,type = "l")
```

```{r experiment on neural network}
# library(keras)
# #library(EBImage)
# library(stringr)
# library(pbapply)
# library(tensorflow)
# use_condaenv("keras-tf", required = T)
# train_array %>%
# layer_dropout(rate = 0.25) %>%
# layer_flatten() %>%
# layer_dense(units = 50, activation = "relu") %>%
# layer_dropout(rate = 0.25) %>%
# layer_dense(units = 1, activation = "sigmoid")
# import tensorflow as tf

# # load library
# require(neuralnet)
# 
# # fit neural network
# nn=neuralnet(expert_labels ~ NDAI + SD + CORR,data=train_data, hidden=3,act.fct = "logistic",
#                 linear.output = T)

# plot(nn)
# pred = compute(nn,test_data)
# prob = pred$net.result[,2]
# pred =ifelse(prob>0.5, 1, 0)
# sum(pred == test_data$expert_labels)/length(test_data$expert_labels)
```


```{r cv master function}
CVmaster = function(classifier,features,labels,nfold = 5){
  #fitControl <- trainControl(## 5-fold CV
                           #method = "cv",
                           #number = nfold)
  folds <- createFolds(labels[[1]], k = nfold)
  fold_errors = c()
  i = 1
  if (classifier=='lda'){
   for(fold in folds){
     train_feature = features[-fold,]
     test_feature = features[fold,]
     train_label = as.factor(labels[-fold,])
     test_label = as.factor(labels[fold,])
     train_set = cbind(train_feature,train_label)
     t = cbind(test_feature,test_label)
     modellda = lda(train_label ~ ., newdata=train_set)
     predict_label = predict(modellda,data=t)
     print(length(predict_label$class))
     print(length(test_label))
     error = (sum(predict_label$class == test_label))/length(test_label)
     #print(predict_label$class )
     #print(unique(test_label))
     fold_errors = c(fold_errors,error)
     print(paste0('This is fold', i))
     i = i+1
  }
  return(fold_errors)
  }
}
```

```{r for cv master}
#cv master on lda regression
traindata <- rbind(train_data,validation_data)
features = traindata %>% 
  dplyr::select(-expert_labels) %>% 
  dplyr::select(NDAI,SD,CORR,DF)
labels = traindata %>% 
  dplyr::select(expert_labels) 
#labels = as.factor(labels[[1]])
CVmaster(classifier='lda', features=features, labels=labels,nfold=5)
labels
```

# Ridge regression with tuned lambda
```{r}
power =seq(-2,-1,by=0.1)
lambdalist = 10^power
cv_ridge_errors = c()
for (l in lambdalist){
  error = CVmaster(classifier='ridge', features=features, labels=labels,nfold=5,l=l)
  cv_ridge_errors =c(cv_ridge_errors,error)
}


```

```{r}
power =seq(-2,-1,by=0.1)
lambdalist = 10^power
lambdalist[which.min(error_ridge)]
```

```{r ridge cv visualize}
ridgedt = as.data.frame(cbind(lambdalist,error_ridge))
theme_update(plot.title = element_text(hjust = 0.5))
ggplot(data=ridgedt, aes(x=lambdalist,y=error_ridge))+
  labs(x='Lambda',y='Average Test Error Rate')+
  geom_line()+
  ggtitle('Average Test Error Acorss All Folds with Different Lambda Values')
```
According to the tuning result, we will obtain lowest test error when we have our $\lambda$ sets to $0.01$. We now perform ridge regression again after parameter tuning. The misclassification error rate for ridge regression is approximately 11.14%.

```{r}
ridge.train = data.matrix(train_data[,4:7])
ridge.label = train_data$expert_labels
ridge.model = glmnet(ridge.train,ridge.label,family=c('binomial'),
                  alpha=0,lambda=0.01,standardize =TRUE)
```

The assumptions for ridge regression remains the same as OLS except we are not requiring the distribution of error to be a guassian distribution. 
We need to check for linearity, constant variance and independence. We can simply fit an model on logistic regression and check assumptions there.

```{r}
logistic.model = glm(expert_labels~NDAI+SD+CORR+DF,data=train_data,family=c('binomial'))
par(mfrow=c(2,2))
plot(logistic.model)
```
Confusion Matrix:
```{r}
ridge.predict = predict(ridge.model,newx=data.matrix(test_data[,4:7]),type='response')
ridge.predict = ifelse(ridge.predict<0.5,0,1)
confusionMatrix(as.factor(test_data$expert_labels),as.factor(ridge.predict))
```

# KNN 

## knn with selected 4 features 
```{r}
set.seed(521)
# knn data rescale
train_data = rbind(train_data,validation_data)
train_knn = scale(train_data[,4:7],center = TRUE,scale = TRUE)
test_knn = scale(test_data[,4:7],center = TRUE,scale = TRUE)
train_labels = train_data$expert_labels

knn_cv = function(train_data,train_label,nfold = 5,k = 5,seed = 521){
  set.seed(521)
  train_knn = scale(train_data,center = TRUE,scale = TRUE)
  folds <- createFolds(train_label, k = nfold)
  errors = c()
  idx = 1
  for (fold in folds){
      train_curr = train_knn[-fold,]
      test_curr = train_knn[fold,]
      knn_curr = knn(train = train_curr,test = test_curr,cl = train_label[-fold],k = k)
      error = 1- sum(knn_curr == train_label[fold])/length(train_label[fold])
      errors = c(errors,error)
      print(paste0("fold ",idx," in session"))
      idx = idx + 1
  }
  return(errors)
}
knn_cv(data.matrix(train_data[,4:7]),train_labels,nfold = 2)

train_labels[1]


ks = c(1,5,9,10,13,15,20,25,30,35)
cv_errors = c()
for(k in ks){
  error = knn_cv(data.matrix(train_data[,4:7]),train_labels,nfold = 5,k = k)
  cv_errors =c(cv_errors,mean(error))
}

knn_block <- plot(ks,cv_errors,type = "l",main = "KNN CV loss",ylab = "Test Loss", xlab = "Choice of K")

ks[which.min(cv_errors)]
knn_cv(data.matrix(train_data[,4:7]),train_labels,nfold = 5,k = 25)

mean(c(0.04917028,0.04855765,0.05111336,0.05183528,0.05156616))
knn.best <- knn(train_knn,test_knn,cl = train_labels,k = 25)
1-sum(knn.best == test_data$expert_labels)/length(knn.best)
confusionMatrix(as.factor(test_data$expert_labels),as.factor(knn.best))
```
## pca
```{r}
library(stats)
set.seed(521)
pca_data = scale(train_data[,4:11],scale = T,center = T)
pca = prcomp(pca_data)
pca$x



#select first 4 pcs 
pca_dataset = pca$x[,1:4]
#xgboost
xgb <- xgboost(data = as.matrix(pca_dataset), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 1,
 max_depth = 4, 
 nround=200, 
 subsample = 0.5,
 colsample_bytree = 0.5,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 verbose = T
)
# form pca test set
pca_data_test = scale(test_data[,4:11],scale = T,center = T)
test_pca = prcomp(pca_data_test)$x
test_pca_dataset = test_pca[,1:4]

y_pred <- predict(xgb, test_pca_dataset)
y_pred <- ifelse(y_pred > 0.5,1,0)
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)



# use first four features(conclusion from eda)
xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
 label = as.numeric(train_data$expert_labels) - 1, 
 eta = 1,
 max_depth = 4, 
 nround=50, 
 subsample = 0.5,
 colsample_bytree = 0.5,
 seed = 1,
 eval_metric = "error",
 objective = "binary:logistic",
 nthread = 1,
 early_stop_rounds = 3,
 verbose = T
)
xgb
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)


## knn 
cv_knn(pca_dataset,train_labels)
cv_knn(train_knn,train_labels)
```

## tuning for xgboost


# 1 
min_child_weight:minimum sum of weights of all observations required in a child
If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. Alternatively, it can be viewed as a commend for algorithm to stop split when node reach a certain level of purity. An useful reference post \:https://stats.stackexchange.com/questions/317073/explanation-of-min-child-weight-in-xgboost-algorithm

max_depth: maximium depth of a tree can get

Both of these two parameters prevent overfiting by stop tree splitting at certain level. Hence, they are tuned toghter here.



```{r}
set.seed(521)
cv_xgboost = function(train_data,label,max_depth = 4,min_child = 1,nfold = 5){
  folds <- createFolds(label, k = nfold, list = TRUE, returnTrain = FALSE)
  errors = c()
  for(fold in folds){
    train_curr = train_data[-fold,]
    test_curr = train_data[fold,]
    label_curr = label[-fold]
    xgb <- xgboost(data = train_curr, 
      label = label_curr, 
      eta = 1,
      max_depth = max_depth, 
      min_child_weight = min_child,
      nround=50, 
      subsample = 0.5,
      colsample_bytree = 0.5,
      eval_metric = "error",
      objective = "binary:logistic",
      nthread = 1,
      verbose = F
    )
    y_pred <- predict(xgb, data.matrix(test_curr))
    y_pred <- ifelse(y_pred > 0.5,1,0)
    error = 1-  sum(y_pred == label[fold])/length(label[fold])
    errors = c(errors,error)
  }
  return(mean(errors))
}

cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1)
max_depth = c(3,5,7,9,11)
min_child = c(1,3,5)

res = c()
for(d in max_depth){
  for(c in min_child){
    error = cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1,max_depth = d,min_child = c)
    res = c(res,error)
    print(paste0("max depths: ",d," mini_child_weight: ",c," ",": ",error))
  }
}

xgb <- xgboost(data = as.matrix(train_data[,4:7]), 
      label = as.numeric(train_data$expert_labels) - 1, 
      eta = 0.1,
      max_depth = 3, 
      min_child_weight = 7,
      nround=50, 
      subsample = 0.5,
      colsample_bytree = 0.5,
      eval_metric = "error",
      objective = "binary:logistic",
      nthread = 1,
      verbose = T,
    )
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
min(res)
confusionMatrix(as.factor(y_pred),as.factor(test_data$expert_labels)) 
```


```{r}
explainer_1 <- explain_xgboost(xgb,
                               data = as.matrix(train_data[,4:7]),
                               y =  as.numeric(train_data$expert_labels) - 1,
                               encode_function = function(data) {
 as.matrix(createDummyFeatures(data))
})

```

```{r}
xgb.plot.tree(model = xgb,trees = 49)
```


# 2 lambda
Although previous part does not signal a strong sign of overfitting, it is notable that xgboost allows adding straightforward regularized term just as in ridge or lasso. We believe it is meaningful to explore the performance of such regularization that is not commonly seen in my tree-based methods.
```{r}
lambda = c(1:10)
res =c()
for(l in lambda){
  
}
```


```{r}
library("xgboost")
library("DALEXtra")
library("mlr")
explainer_1 <- explain_xgboost(xgb, 
                               data = train_data[,4:7],
                               y = train_data$expert_labels,
                               encode_function = function(data) {
 as.matrix(createDummyFeatures(data))
})
explainer_1$data

res = predict_parts(explainer_1, test_data[328,4:7])
res
plot(res)
for(i in 2:nrow(test_data)){
  res = cbind(res,predict_parts(explainer_1, test_data[i,4:7]))
  print(i)
}

```

