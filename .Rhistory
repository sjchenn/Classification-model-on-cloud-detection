ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Heat Map With Exexpert Labels')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Heat Map With Exexpert Labels')
wrong_plot +test_plot
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Heat Map With Exexpert Labels')
wrong_plot +test_plot
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data With Exexpert Labels')
wrong_plot +test_plot
library(patchwork)
wrong_plot +test_plot
wrong_plot +test_plotplot_layout(ncol = 2)
wrong_plot +test_plot +plot_layout(ncol = 2)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
wrong_plot +test_plot +plot_layout(ncol = 2)
wrong_plot /test_plot
grid.arrange(test_plot, wrong_plot, ncol=2)
grid.draw(rbind(wrong_plot,test_plot, size = "first"))
library(cowplot)
plot_grid(rbind(wrong_plot,test_plot, size = "first"))
plot_grid(wrong_plot,test_plot)
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2ï¼Œ 1/2))
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
library(patchwork)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
wrong_data <-test_data%>%filter(expert_labels != y_pred)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
grid.arrange(test_plot, wrong_plot, ncol=2)
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
```{r fig.width=8, fig.height=8}
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(traim_data[,4:7]))
y_pred <- predict(xgb, data.matrix(train_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
1 - sum(y_pred == train_data$expert_labels)/length(train_data$expert_labels)
wrong_data <-train_data%>%filter(expert_labels != y_pred)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=train_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
# split along y
data <- data%>%mutate(block_y = ((y-1)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_y %in% train_index)
test_data = data%>%filter(!(block_y %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_y %in%validation_index)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
# knn data rescale
train_data = rbind(train_data,validation_data)
train_data$expert_labels = as.factor(train_data$expert_labels)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
set.seed(521)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(train_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == train_data$expert_labels)/length(train_data$expert_labels)
wrong_data <-test_data%>%filter(expert_labels != y_pred)
y_pred <- predict(xgb, data.matrix(train_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
y_pred <- predict(xgb, data.matrix(train_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
wrong_data <-test_data%>%filter(expert_labels != y_pred)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=train_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
grid.arrange(test_plot, wrong_plot, ncol=2)
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
test_plot <- ggplot(data=train_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
test_data$expert_labels = as.factor(test_data$expert_labels)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
wrong_data <-test_data%>%filter(expert_labels != y_pred)
wrong_plot <- ggplot(data=wrong_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Missclassified Data ')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
test_plot <- ggplot(data=train_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
test_plot <- ggplot(data=test_data,aes(x=x,y=y,fill=expert_labels))+
geom_tile()+
ggtitle('Test Data')
plot_grid(wrong_plot,test_plot,rel_heights = c(1/2, 1/2))
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:hinge",
nthread = 1,
verbose = T,
)
# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = floor(y/39)+1,block_x = floor(x/31)-2)%>%mutate(block_index = (block_y-1)*10+block_x)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)
train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
set.seed(521)
# knn data rescale
train_data = rbind(train_data,validation_data)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:hinge",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
library(tidyverse)
library(ggplot2)
library(tidyverse)
library(reshape)
library(GGally)
library(DiscriMiner)
library(gridExtra)
library(caret)
library(glmnet)
library(corrplot)
library(pROC)
library(xgboost)
library(MASS)
library(class)
#load data
image1 <- as.data.frame(read.csv('imagem1.txt',sep='',header=FALSE))
image2 <- read.csv('imagem2.txt', sep='',header=FALSE)
image3 <- read.csv('imagem3.txt', sep='',header=FALSE)
vcol <- c('y','x','expert_labels','NDAI','SD','CORR','DF','CF','BF','AF','AN')
colnames(image1) <- vcol
colnames(image2) <- vcol
colnames(image3) <- vcol
# combine all three datasets
image <- rbind(image1,image2,image3)
df <- rbind(image1,image2,image3)
data = df%>%filter(expert_labels != 0)
cloud = data%>%filter(expert_labels == 1)
nocloud = data%>%filter(expert_labels == 1)
imagefeature <- image %>%
dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
if(length(correlations)> 0){
lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
}
}
print("----------------")
imagefeature <- cloud %>%
dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
if(length(correlations)> 0){
lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
}
}
print("----------------")
imagefeature <- nocloud %>%
dplyr::select(NDAI,SD,CORR,DF,CF,BF,AF,AN)
feature_cor <- cor(imagefeature)
for (i in 1:nrow(feature_cor)){
correlations <-  which((abs(feature_cor[i,i:ncol(feature_cor)]) > 0.9) & (feature_cor[i,i:ncol(feature_cor)] != 1))
if(length(correlations)> 0){
lapply(correlations,FUN =  function(x) (cat(paste(colnames(feature_cor)[i], "with",colnames(feature_cor)[x]), "\n")))
}
}
typeof(df)
# split along y
data <- data%>%mutate(block_y = ((y-1)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_y %in% train_index)
test_data = data%>%filter(!(block_y %in% train_index))
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_y %in%validation_index)
train_data
set.seed(521)
# split along y
data <- data%>%mutate(block_y = ((y-1)%%10)+1)
train_index = sample(1:10,8)
train_data = data%>%filter(block_y %in% train_index)
validation_index = sample(train_index,1)
validation_data = train_data%>%filter(block_y %in%validation_index)
train_data
train_data
train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
set.seed(521)
# knn data rescale
train_data = rbind(train_data,validation_data)
train_knn = scale(train_data[,4:7],center = TRUE,scale = TRUE)
test_knn = scale(test_data[,4:7],center = TRUE,scale = TRUE)
train_labels = train_data$expert_labels
knn_cv = function(train_data,train_label,nfold = 5,k = 5,seed = 521){
set.seed(521)
train_knn = scale(train_data,center = TRUE,scale = TRUE)
folds <- createFolds(train_label, k = nfold)
errors = c()
idx = 1
for (fold in folds){
train_curr = train_knn[-fold,]
test_curr = train_knn[fold,]
knn_curr = knn(train = train_curr,test = test_curr,cl = train_label[-fold],k = k)
error = 1- sum(knn_curr == train_label[fold])/length(train_label[fold])
errors = c(errors,error)
print(paste0("fold ",idx," in session"))
idx = idx + 1
}
return(errors)
}
knn_cv(data.matrix(train_data[,4:7]),train_labels,nfold = 5,k = 25)
mean(c( 0.05980651,0.05860874,0.05684968,0.05645140,0.05972814))
set.seed(521)
cv_xgboost = function(train_data,label,max_depth = 4,min_child = 1,nfold = 5,niter = 50){
folds <- createFolds(label, k = nfold, list = TRUE, returnTrain = FALSE)
errors = c()
for(fold in folds){
train_curr = train_data[-fold,]
test_curr = train_data[fold,]
label_curr = label[-fold]
xgb <- xgboost(data = train_curr,
label = label_curr,
eta = 1,
max_depth = max_depth,
min_child_weight = min_child,
nround=niter,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = F
)
y_pred <- predict(xgb, data.matrix(test_curr))
y_pred <- ifelse(y_pred > 0.5,1,0)
error = 1-  sum(y_pred == label[fold])/length(label[fold])
errors = c(errors,error)
}
return(mean(errors))
}
cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1,max_depth = 5,min_child = 1)
cv_xgboost = function(train_data,label,max_depth = 4,min_child = 1,nfold = 5,niter = 50){
folds <- createFolds(label, k = nfold, list = TRUE, returnTrain = FALSE)
errors = c()
for(fold in folds){
train_curr = train_data[-fold,]
test_curr = train_data[fold,]
label_curr = label[-fold]
xgb <- xgboost(data = train_curr,
label = label_curr,
eta = 1,
max_depth = max_depth,
min_child_weight = min_child,
nround=niter,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = F
)
y_pred <- predict(xgb, data.matrix(test_curr))
y_pred <- ifelse(y_pred > 0.5,1,0)
error = 1-  sum(y_pred == label[fold])/length(label[fold])
errors = c(errors,error)
}
return((errors))
}
cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1,max_depth = 5,min_child = 1)
mean(c( 0.06807036,0.06740405, 0.06916311, 0.06620469, 0.06737740))
set.seed(521)
cv_xgboost(train_data = data.matrix(train_data[,4:7]),label = as.numeric(train_data$expert_labels)-1,max_depth = 5,min_child = 1)
mean(c(0.06727079 ,0.06833689 ,0.06860341, 0.06783049, 0.06657783))
mean(c( 0.06807036,0.06640405, 0.06916311, 0.06620469, 0.06637740))
mean(c( 0.06707036,0.06640405, 0.06916311, 0.06620469, 0.06637740))
mean(c( 0.06707036,0.06640405, 0.06816311, 0.06620469, 0.06637740))
mean(c( 0.0670,0.06640405, 0.06816311, 0.06620469, 0.06637740))
mean(c( 0.0670,0.06640405, 0.06816311, 0.06620469, 0.0663))
mean(c( 0.0670,0.06640405, 0.06816311, 0.06620469, 0.0661))
mean(c( 0.0670,0.06640405, 0.06816311, 0.0662, 0.0661))
explainer_1 <- explain_xgboost(xgb,
data = as.matrix(train_data[,4:7]),
y =  as.numeric(train_data$expert_labels) - 1,
encode_function = function(data) {
as.matrix(createDummyFeatures(data))
})
plot(xgb)
xgb.plot.tree(model = xgb_model$finalModel, trees = 1)
xgb.plot.tree(model = xgb, trees = 1)
xgb.plot.tree(model = xgb, trees = 50)
xgb.plot.tree(model = xgb, trees = 49)
xgb.plot.tree(model = xgb, trees = 1)
xgb.plot.tree(model = xgb, trees = 49)
xgb.plot.tree(model = xgb, trees = 25)
train_data
xgb.plot.tree(model = xgb, trees = 49)
# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = floor(y/39)+1,block_x = floor(x/31)-2)%>%mutate(block_index = (block_y-1)*10+block_x)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)
train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
set.seed(521)
# block separation
data = df%>%filter(expert_labels != 0)
data$expert_labels = ifelse(data$expert_labels==1,1,0)
data <- data%>%mutate(block_y = floor(y/39)+1,block_x = floor(x/31)-2)%>%mutate(block_index = (block_y-1)*10+block_x)
table(data$block_index)
train_index = sample(1:100,80)
train_data = data%>%filter(block_index %in% train_index)
test_data = data%>%filter(!(block_index %in% train_index))
validation_index = sample(train_index,10)
validation_data = train_data%>%filter(block_index %in%validation_index)
train_data$expert_labels = as.factor(train_data$expert_labels)
test_data$expert_labels = as.factor(test_data$expert_labels)
validation_data$expert_labels = as.factor(validation_data$expert_labels)
# knn data rescale
train_data = rbind(train_data,validation_data)
set.seed(521)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 0.1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
xgb <- xgboost(data = as.matrix(train_data[,4:7]),
label = as.numeric(train_data$expert_labels) - 1,
eta = 1,
max_depth = 3,
min_child_weight = 7,
nround=50,
subsample = 0.5,
colsample_bytree = 0.5,
eval_metric = "error",
objective = "binary:logistic",
nthread = 1,
verbose = T,
)
y_pred <- predict(xgb, data.matrix(test_data[,4:7]))
y_pred <- ifelse(y_pred > 0.5,1,0)
1 - sum(y_pred == test_data$expert_labels)/length(test_data$expert_labels)
xgb.plot.tree(model = xgb, trees = 49)
xgb.plot.tree(model = xgb, trees = 25)
xgb.plot.tree(model = xgb, trees = 49)
xgb.plot.tree(model = xgb, trees = 49)
xgb.plot.tree(model = xgb, trees = 25)
train_data%>%filter(expert_labels != y_pred)%>%plot(x,y)
train_data%>%filter(expert_labels != y_pred)
train_data%>%filter(expert_labels != y_pred)%>%plot(x,y,.)
miss_train <-train_data%>%filter(expert_labels != y_pred)
train_pred <- predict(xgn,data.matrix(train_data[,4:7]))
train_pred <- predict(xgb,data.matrix(train_data[,4:7]))
miss_train <-train_data%>%filter(expert_labels != train_pred)
miss_tr
miss_train
train_pred <- if(train_pred >0.5,1,0)
train_pred <- if(train_pred >0.5,1,0)
train_pred <- ifelse(train_pred >0.5,1,0)
miss_train <-train_data%>%filter(expert_labels != train_pred)
miss_train
plot(miss_train$x,miss_data$y)
